#!/bin/bash

# specify the input file containing the URLs
input_file="urls.txt"

# loop through each URL in the input file
while read url
do
    # use katana to crawl the URL and save the data to a file
    katana -u $url -o ${url}_katana.txt

    # use dirhunt to crawl the URL and save the data to a file
    dirhunt $url | tee ${url}_dirhunt.txt

    # use jsfinder to crawl the URL and save the data to a file
    jsfinder -u $url -o ${url}_jsfinder.txt

    # use hakrawler to crawl the URL and save the data to a file
    hakrawler -url $url -plain -depth 2 -o ${url}_hakrawler.txt

    # combine the output of all the tools and remove duplicate URLs
    cat ${url}_katana.txt ${url}_dirhunt.txt ${url}_jsfinder.txt ${url}_hakrawler.txt | sort -u > ${url}_combined.txt

    # check the status code of each URL in the final output file using httpx
    while read line
    do
        # retrieve the status code using the httpx command
        status=$(httpx -s -head $line | grep "HTTP/" | awk '{print $2}')

        # print the URL and status code to the console
        echo "$line: $status"
    done < ${url}_combined.txt
done < "$input_file"
