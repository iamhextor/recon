#!/bin/bash

function crawl_url {
    # extract the domain name from the input URL
    domain=$(echo $1 | sed 's|https://||;s|http://||;s|www.||;s|/.*||')

    # use katana to crawl the URL and save the data to a file
    katana -u $1 -silent -o "${domain}_katana.txt"

    # use dirhunt to crawl the URL and save the data to a file
    dirhunt $1 --to-file "${domain}_dirhunt.txt"
    sed -i '1,/\"processing\"/d; s/^[ \t]*//; s/[",]//g; /]/,$d' "${domain}_dirhunt.txt"

    # use hakrawler to crawl the URL and save the data to a file
    echo "$1" | hakrawler | tee "${domain}_hakrawler.txt" 
    sed -i 's/\[script\] //g; s/\[form\] //g; s/\[href\] //g' "${domain}_hakrawler.txt" 
	
	# Run gau and save output to url_gau.txt
    gau "${domain}" --o="${domain}_gau.txt"
	
	# Run waybackurls and save output to url_waybackurls.txt
    echo "${domain}" | waybackurls > "${domain}_waybackurls.txt"
	
	# Run waymore.py and save output to url_waymore.txt
    python3 waymore/waymore.py -i "${domain}" -mode=U -xcc -oU="${domain}_waymore.txt"
	
    # combine the output of all the tools and remove duplicate URLs
    cat "${domain}_katana.txt" "${domain}_dirhunt.txt" "${domain}_hakrawler.txt" "${domain}_gau.txt" "${domain}_waybackurls.txt" "${domain}_waymore.txt" | grep $url | sort -u > "${domain}_combined.txt"
    
    # check the status code of each URL in the final output file using httpx and save the output in Live_Crawl.txt
    while read line
    do
        # retrieve the status code using the httpx command
        status=$(httpx -u $line)

        # print the URL and status code to the console
        echo "$line: $status"

        # save the URL and status code to the Live_Crawl.txt file
        echo "$line: $status" >> "${domain}Live_Crawl.txt"
    done < "${domain}_combined.txt"

    # remove the temporary files
    # rm "${domain}_katana.txt" "${domain}_dirhunt.txt" "${domain}_hakrawler.txt" "${domain}_combined.txt"
}

# specify the input file containing the URLs
input_file="urls.txt"

# loop through each URL in the input file
while read url
do
    crawl_url $url
done < "$input_file"
